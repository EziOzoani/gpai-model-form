{
  "model_name": "Meta Llama 3.2 90B",
  "provider": "Meta",
  "region": "US",
  "size": "Big",
  "release_date": "2024-09-25",
  "transparency_score": {
    "overall": 100,
    "sections": {
      "general": 1.0,
      "properties": 1.0,
      "distribution": 1.0,
      "use": 1.0,
      "data": 1.0,
      "training": 1.0,
      "compute": 1.0,
      "energy": 0.0
    }
  },
  "stars": 2,
  "label_x": "US-Big",
  "last_updated": "2025-11-11 17:43:13",
  "section_data": {
    "general": {
      "content": "Meta Platforms, Inc., formerly Facebook, invests heavily in open AI research through its Fundamental AI Research team. The company advocates for open-source AI development to accelerate innovation. Llama 3.2 90B remains unavailable in EU markets due to regulatory considerations.",
      "sources": [
        {
          "url": "https://ai.meta.com/llama",
          "type": "official",
          "confidence": 0.95
        },
        {
          "url": "https://ai.meta.com/blog/llama-3-2",
          "type": "blog",
          "confidence": 0.9
        }
      ]
    },
    "properties": {
      "content": "Llama 3.2 90B implements an efficient Transformer architecture with multimodal capabilities for processing text and images. The model incorporates rotary position embeddings and grouped-query attention for optimal performance. Design choices prioritise inference efficiency on consumer hardware.",
      "sources": [
        {
          "url": "https://ai.meta.com/research/publications/llama-3",
          "type": "technical",
          "confidence": 0.9
        }
      ]
    },
    "distribution": {
      "content": "Models are distributed under Meta's custom Llama licence permitting commercial use with restrictions. Direct downloads available from Meta's portal with cloud deployments through major providers. Distribution requires acceptance of licence terms including usage reporting for large deployments.",
      "sources": [
        {
          "url": "https://llama.meta.com/get-started",
          "type": "documentation",
          "confidence": 0.95
        }
      ]
    },
    "use": {
      "content": "The Llama licence permits commercial use whilst prohibiting certain applications including illegal activities and harm. Usage restrictions apply to organisations with over 700 million monthly active users. Compliance with Meta's responsible use guide is mandatory for all deployments.",
      "sources": [
        {
          "url": "https://llama.meta.com/llama3/license",
          "type": "legal",
          "confidence": 1.0
        }
      ]
    },
    "data": {
      "content": "Training data encompasses extensive web content, open-source code repositories, and academic publications. Meta implements sophisticated filtering to ensure data quality and remove personally identifiable information. The dataset represents one of the largest curated collections for language model training.",
      "sources": [
        {
          "url": "https://ai.meta.com/blog/meta-llama-3",
          "type": "blog",
          "confidence": 0.8
        }
      ]
    },
    "training": {
      "content": "Training employed large-scale pretraining across Meta's extensive GPU clusters with careful optimisation for efficiency. The process incorporated curriculum learning and data mixture adjustments throughout training phases. Safety fine-tuning followed initial pretraining to align model behaviour.",
      "sources": [
        {
          "url": "https://arxiv.org/abs/2307.09288",
          "type": "research",
          "confidence": 0.85
        }
      ],
      "bonus_star": true
    },
    "compute": {
      "content": "Model training required approximately three months on Meta's Research SuperCluster infrastructure. Training utilised thousands of GPUs with custom interconnects for efficient distributed computation. Hardware optimisations reduced training time compared to previous Llama versions.",
      "sources": [
        {
          "url": "https://ai.meta.com/blog/ai-rsc",
          "type": "technical",
          "confidence": 0.75
        }
      ],
      "bonus_star": true
    },
    "energy": {
      "content": "Energy consumption details not disclosed publicly.",
      "sources": []
    }
  }
}