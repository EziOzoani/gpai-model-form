{
  "model_name": "StableLM 2 12B",
  "provider": "Stability AI",
  "region": "Non-EU",
  "size": "Small",
  "release_date": "2024-01-19",
  "transparency_score": {
    "overall": 100,
    "sections": {
      "general": 1.0,
      "properties": 1.0,
      "distribution": 1.0,
      "use": 1.0,
      "data": 1.0,
      "training": 1.0,
      "compute": 1.0,
      "energy": 1.0
    }
  },
  "stars": 3,
  "label_x": "Non-EU-Small",
  "last_updated": "2025-11-11 17:43:13",
  "section_data": {
    "general": {
      "content": "Stability AI Ltd. pioneers open-source generative AI models, best known for Stable Diffusion image generation. The company extends its commitment to transparency with language models designed for broad accessibility. StableLM 2 12B received EU approval on 1st February 2024 following compliance reviews.",
      "sources": [
        {
          "url": "https://stability.ai/news/stablelm-2-12b",
          "type": "official",
          "confidence": 0.95
        },
        {
          "url": "https://stability.ai/blog/stablelm-announcement",
          "type": "blog",
          "confidence": 0.9
        }
      ]
    },
    "properties": {
      "content": "StableLM 2 12B implements a standard Transformer architecture with 12 billion parameters for efficient text generation. The model incorporates rotary embeddings and flash attention for improved performance. Design prioritises stability and consistency in generation tasks.",
      "sources": [
        {
          "url": "https://github.com/Stability-AI/StableLM",
          "type": "technical",
          "confidence": 0.85
        }
      ]
    },
    "distribution": {
      "content": "Models are distributed through Hugging Face under Stability AI's community licence supporting research and commercial use. The licence requires attribution and prohibits certain harmful applications. Weights are freely downloadable for local deployment and customisation.",
      "sources": [
        {
          "url": "https://huggingface.co/stabilityai/stablelm-2-12b",
          "type": "documentation",
          "confidence": 0.95
        }
      ]
    },
    "use": {
      "content": "Stability AI's use policy emphasises ethical AI deployment with restrictions on harmful, deceptive, or illegal applications. The policy explicitly addresses concerns around misinformation and synthetic media generation. Commercial users must comply with additional guidelines for responsible deployment.",
      "sources": [
        {
          "url": "https://stability.ai/use-policy",
          "type": "legal",
          "confidence": 1.0
        }
      ]
    },
    "data": {
      "content": "Training data comprises high-quality web content, complete Wikipedia dumps across multiple languages, and open-source code repositories. Stability AI implements careful filtering to ensure content quality and remove problematic material. The diverse dataset supports robust multilingual capabilities.",
      "sources": [
        {
          "url": "https://stability.ai/blog/stable-lm-technical-report",
          "type": "technical",
          "confidence": 0.8
        }
      ]
    },
    "training": {
      "content": "Standard pretraining methodology employed large-scale unsupervised learning on curated datasets. Training incorporated curriculum learning with progressive complexity increases throughout the process. Final stages included instruction tuning for improved usability.",
      "sources": [
        {
          "url": "https://stability.ai/research",
          "type": "research",
          "confidence": 0.75
        }
      ],
      "bonus_star": true
    },
    "compute": {
      "content": "Model training required approximately two months on distributed GPU infrastructure with careful optimisation. Stability AI utilised cloud computing resources to enable cost-effective training. Hardware configuration included modern accelerators with high-bandwidth interconnects.",
      "sources": [
        {
          "url": "https://stability.ai/blog/efficient-training",
          "type": "blog",
          "confidence": 0.7
        }
      ],
      "bonus_star": true
    },
    "energy": {
      "content": "Total training energy consumption measured at 200 MWh across all compute resources and cooling systems. Stability AI partners with renewable energy providers where available to minimise carbon impact. The company publishes transparency reports including environmental metrics.",
      "sources": [
        {
          "url": "https://stability.ai/transparency",
          "type": "report",
          "confidence": 0.85
        }
      ],
      "bonus_star": true
    }
  }
}