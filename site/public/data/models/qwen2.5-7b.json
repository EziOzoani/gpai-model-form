{
  "model_name": "Qwen2.5 7B",
  "provider": "Alibaba",
  "region": "Non-EU",
  "size": "Small",
  "release_date": "2024-09-19",
  "transparency_score": {
    "overall": 60,
    "sections": {
      "general": 1.0,
      "properties": 1.0,
      "distribution": 1.0,
      "use": 0.0,
      "data": 0.0,
      "training": 0.0,
      "compute": 0.0,
      "energy": 0.0
    }
  },
  "stars": 0,
  "label_x": "Non-EU-Small",
  "last_updated": "2025-11-11 17:43:13",
  "section_data": {
    "general": {
      "content": "Alibaba Cloud leads Chinese AI development through its DAMO Academy research division. The company invests heavily in multilingual models supporting global business applications. Qwen2.5 7B currently lacks EU availability pending regulatory compliance assessments.",
      "sources": [
        {
          "url": "https://www.alibabacloud.com/ai/qwen",
          "type": "official",
          "confidence": 0.9
        },
        {
          "url": "https://qwenlm.github.io",
          "type": "announcement",
          "confidence": 0.85
        }
      ]
    },
    "properties": {
      "content": "Qwen2.5 7B utilises an optimised Transformer architecture with 7 billion parameters supporting extensive multilingual capabilities. The model excels at Chinese language understanding whilst maintaining strong English performance. Architecture refinements focus on efficient tokenisation for Asian languages.",
      "sources": [
        {
          "url": "https://arxiv.org/abs/2309.16609",
          "type": "technical",
          "confidence": 0.85
        }
      ]
    },
    "distribution": {
      "content": "Released under Apache 2.0 licence enabling open commercial use and modification without restrictions. Available through ModelScope platform for Chinese users and Hugging Face for international access. Alibaba Cloud provides optimised deployments through its cloud services.",
      "sources": [
        {
          "url": "https://modelscope.cn/models/qwen/Qwen2.5-7B",
          "type": "documentation",
          "confidence": 0.9
        }
      ]
    },
    "use": {
      "content": "Usage policy details not publicly disclosed.",
      "sources": []
    },
    "data": {
      "content": "Training data information remains confidential.",
      "sources": []
    },
    "training": {
      "content": "Training methodology not published.",
      "sources": []
    },
    "compute": {
      "content": "Compute infrastructure details not available.",
      "sources": []
    },
    "energy": {
      "content": "Energy consumption metrics not disclosed.",
      "sources": []
    }
  }
}